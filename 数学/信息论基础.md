# 信息论基础(信息量、熵、KL散度、交叉熵)

[toc]

我们知道同为正态分布的两个模型可以通过均值和方差进行模型之间的差异比较。那不同类别的模型，比如正态分布和泊松分布之间应该如何衡量差异呢？

答：只要是两个概率模型，就能被**熵**($Entropy$)统一衡量。

**熵：在信息论中用来表示随机变量不确定性的度量。**

---

## 1. 信息量与熵

先引入自信息/信息量：

**自信息**$self\ introduction$：表示**一个**随机事件所包含的**信息量**，一个随机事件发生的概率越高，其自信息越低；必然事件的自信息为0。
> **信息量是指信息多少的度量**（一个事件从不确定变为确定，他的难度有多大）。比如中国足球队夺冠概率为$\frac{1}{1024}$，你却告诉我他夺冠了，那么这个"信息"就有10bit的信息量。

- 随机变量$X$，当$X=x_i$时的自信息 / 信息量$I(x)$定义为：$$I(x_i) = -log\ p_i$$
- > $p_i为X取值为x_i$的概率;
  > 对数的底数可以为2、$e$和10，为2时单位为bit，为$e$时单位为纳特$nat$。

==**信息量**是**一个具体事件从不确定变为确定的难度有多大**==。信息量越大说明难度越高。
==**熵**类似，但不是衡量一个具体事件，熵用来衡量整个系统中的所有事件，即**一个系统从不确定变为确定的难度有多大**。==

- 对于分布为$p(x)$的随机变量$X$，其**信息量/自信息的期望为熵**。公式为:$$H(X)=E(-log\ p_i)=-\sum_{i=1}^n p_i \ log\ p_i$$

---

## 2. KL散度与交叉熵

ok,回到开始的问题，如何比较两个概率模型。

如果直接算出两个概率模型的熵来比较大小，有些简单粗暴。
那么引入**KL散度/相对熵**

$$
D_{KL}(P||Q)：以P为基准考虑P和Q相差多少
$$

$$
D_{KL}(P||Q) := \sum_{i=1}^m p_i(I_Q(q_i) - I_P(p_i))
= \sum_{i=1}^m p_i((-log_2q_i) - (-log_2p_i))\\
=\sum_{i=1}^m p_i(-log_2q_i) - \sum_{i=1}^m p_i(-log_2p_i)
= H(P,Q) - H(P)
$$
> H(P)就是P的熵，恒定不变；
> H(P,Q)为交叉熵
> KL散度=0时，P和Q最接近

由吉布斯不等式可知：KL散度≥0，QP相等时=0。那么要想让P和Q概率模型非常接近，就需要找到**交叉熵**$H(P,Q)$的最小值。

**交叉熵**公式：
$$
H(P,Q) = -\sum_{i=1}^m p_i\ log q_i
$$

### 交叉熵与$Logistic$回归


**对于$Logistic$回归，假设真实的$label$概率分布为$P(x)$,模型预测的为$Q(x)$，那么就可以用KL散度衡量预测值与真实值概率模型之间的差异**；

KL散度公式：

$$
D_{KL}(P||Q) =  H(P,Q) - H(P)
$$

然后$H(P)$(真实值的熵)为固定值，**所以$H(P,Q)$交叉熵越小，KL散度越大，预测值与真实值之间差异越大，故选取交叉熵作为$logistic$回归的损失函数**。

根据交叉熵公式：
$logistic$回归中$m=2$，得到**二元交叉熵**：
$$
H(P,Q) = -( p_1(logq_1) + p_2(logq_2) )\\
 =  -( p_1(logq_1) + (1-p_1)(log(1-q_2)))\\
 = -(ylog(h_\theta (x)) + (1-y)log(1-h_\theta (x)))
$$

## 后续待补充

参考：
B站 - [王木头学科学](https://www.bilibili.com/video/BV15V411W7VB?spm_id_from=333.999.0.0&vd_source=31f382886b368673a25ce3ff23e82bfc)
神经网络与深度学习 - 邱锡鹏
